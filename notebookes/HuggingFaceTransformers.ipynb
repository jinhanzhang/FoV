{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from transformers import TimeSeriesTransformerForPrediction, TimeSeriesTransformerConfig, TimeSeriesTransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1, 3])\n",
      "tensor([[[4.2641e-01, 3.6268e-02, 2.7871e-01],\n",
      "         [4.0906e-01, 7.8652e-01, 3.5934e-01],\n",
      "         [8.4251e-01, 6.3028e-01, 3.6465e-01],\n",
      "         [4.8450e-01, 8.1085e-01, 1.3139e-01],\n",
      "         [3.0891e-01, 5.0767e-01, 9.7587e-01],\n",
      "         [4.7287e-01, 3.8401e-01, 8.9538e-01],\n",
      "         [7.0356e-02, 8.8449e-01, 1.7575e-01],\n",
      "         [5.4985e-01, 9.0537e-01, 2.7571e-01],\n",
      "         [6.8929e-01, 9.9291e-01, 7.4718e-01],\n",
      "         [8.7458e-01, 6.0551e-01, 3.3010e-01],\n",
      "         [8.7458e-01, 6.0551e-01, 3.3010e-01]],\n",
      "\n",
      "        [[3.6944e-01, 3.7984e-01, 7.2225e-01],\n",
      "         [2.2228e-03, 4.7676e-02, 6.2349e-01],\n",
      "         [1.9012e-01, 3.1139e-01, 8.4146e-01],\n",
      "         [4.4738e-01, 6.4928e-02, 6.4491e-01],\n",
      "         [5.7531e-03, 6.7091e-01, 8.1593e-01],\n",
      "         [9.1024e-01, 3.6144e-01, 1.2201e-01],\n",
      "         [5.4872e-01, 7.9318e-01, 6.2732e-01],\n",
      "         [3.8312e-01, 4.8178e-01, 8.7700e-02],\n",
      "         [3.2606e-01, 8.9937e-01, 2.7801e-01],\n",
      "         [4.5224e-01, 9.0023e-01, 6.6794e-01],\n",
      "         [4.5224e-01, 9.0023e-01, 6.6794e-01]],\n",
      "\n",
      "        [[4.9947e-01, 2.5270e-01, 9.5629e-01],\n",
      "         [1.7345e-01, 2.8286e-01, 5.8264e-01],\n",
      "         [1.9870e-01, 2.0540e-01, 5.1360e-01],\n",
      "         [6.0459e-02, 1.7537e-01, 9.8083e-01],\n",
      "         [5.7452e-01, 1.5982e-01, 5.8188e-01],\n",
      "         [9.5467e-01, 6.4182e-01, 4.8857e-01],\n",
      "         [3.8889e-02, 8.8095e-01, 1.3273e-01],\n",
      "         [8.7317e-01, 1.9806e-01, 8.6177e-01],\n",
      "         [5.6799e-01, 7.7273e-02, 9.4445e-01],\n",
      "         [7.5223e-01, 4.8430e-01, 9.3422e-01],\n",
      "         [7.5223e-01, 4.8430e-01, 9.3422e-01]],\n",
      "\n",
      "        [[9.4091e-01, 6.7545e-01, 4.6738e-02],\n",
      "         [1.5360e-01, 6.7223e-01, 8.9734e-01],\n",
      "         [4.5184e-01, 5.3398e-01, 6.1370e-01],\n",
      "         [3.3639e-01, 6.2203e-01, 1.4720e-01],\n",
      "         [3.8923e-02, 1.6100e-01, 1.5964e-01],\n",
      "         [3.4568e-01, 7.7761e-01, 3.6324e-02],\n",
      "         [6.0172e-01, 5.0087e-02, 9.5273e-01],\n",
      "         [3.8788e-01, 7.3220e-01, 1.5928e-01],\n",
      "         [7.2808e-01, 9.2807e-01, 3.1473e-01],\n",
      "         [5.1966e-01, 4.0833e-01, 2.8691e-01],\n",
      "         [5.1966e-01, 4.0833e-01, 2.8691e-01]],\n",
      "\n",
      "        [[1.5888e-01, 1.8014e-01, 2.1530e-01],\n",
      "         [1.6192e-01, 4.1108e-02, 5.7330e-01],\n",
      "         [7.1306e-01, 3.9683e-02, 9.0163e-01],\n",
      "         [9.0678e-01, 6.4269e-01, 5.3271e-01],\n",
      "         [1.5232e-01, 4.7798e-03, 5.9537e-01],\n",
      "         [7.8584e-01, 4.6243e-01, 4.6289e-01],\n",
      "         [5.1552e-01, 5.7284e-01, 5.3437e-01],\n",
      "         [7.0503e-01, 3.2437e-01, 6.4454e-01],\n",
      "         [6.6525e-02, 6.9135e-01, 6.0736e-02],\n",
      "         [3.0654e-01, 2.1108e-01, 7.8643e-01],\n",
      "         [3.0654e-01, 2.1108e-01, 7.8643e-01]],\n",
      "\n",
      "        [[2.0629e-01, 8.6444e-02, 2.1453e-01],\n",
      "         [3.3102e-01, 2.8771e-01, 9.6676e-01],\n",
      "         [2.0181e-01, 5.7599e-01, 9.5212e-01],\n",
      "         [5.1628e-01, 2.2579e-01, 3.7680e-01],\n",
      "         [9.5501e-01, 1.8765e-01, 2.0339e-01],\n",
      "         [9.4160e-01, 9.5358e-01, 4.6192e-01],\n",
      "         [2.2376e-01, 4.0453e-02, 2.1400e-01],\n",
      "         [5.6932e-01, 2.8219e-01, 8.7017e-01],\n",
      "         [3.8260e-01, 6.3573e-01, 6.5520e-01],\n",
      "         [8.8956e-01, 1.0784e-01, 8.1716e-01],\n",
      "         [8.8956e-01, 1.0784e-01, 8.1716e-01]],\n",
      "\n",
      "        [[5.9906e-01, 8.4883e-02, 6.0298e-01],\n",
      "         [3.1524e-01, 8.2825e-03, 1.7192e-01],\n",
      "         [6.8615e-01, 2.1459e-01, 2.9468e-01],\n",
      "         [9.2157e-01, 7.0998e-01, 3.6459e-01],\n",
      "         [1.9871e-01, 7.1227e-02, 4.7798e-01],\n",
      "         [5.9733e-01, 6.1125e-01, 5.9056e-01],\n",
      "         [8.8020e-01, 1.1908e-01, 7.9100e-01],\n",
      "         [4.2801e-02, 2.5135e-01, 2.8166e-01],\n",
      "         [1.6226e-01, 4.0715e-01, 9.0718e-01],\n",
      "         [9.2572e-01, 5.0282e-01, 3.7293e-01],\n",
      "         [9.2572e-01, 5.0282e-01, 3.7293e-01]],\n",
      "\n",
      "        [[8.6286e-01, 8.9333e-01, 8.3848e-01],\n",
      "         [7.1016e-01, 5.8926e-01, 5.7631e-02],\n",
      "         [2.1882e-01, 1.7307e-01, 4.3427e-01],\n",
      "         [8.0283e-01, 5.8689e-01, 5.1510e-01],\n",
      "         [5.4912e-01, 6.4974e-01, 8.3903e-01],\n",
      "         [3.1800e-01, 8.3032e-01, 5.5058e-02],\n",
      "         [6.8929e-01, 6.2590e-02, 3.7660e-01],\n",
      "         [1.7824e-01, 2.2476e-01, 7.1150e-01],\n",
      "         [5.1665e-01, 6.3623e-01, 1.1174e-01],\n",
      "         [1.7348e-01, 4.1570e-01, 4.4054e-04],\n",
      "         [1.7348e-01, 4.1570e-01, 4.4054e-04]],\n",
      "\n",
      "        [[2.6597e-01, 9.7546e-01, 6.2978e-01],\n",
      "         [9.6652e-01, 6.3961e-01, 3.1782e-01],\n",
      "         [2.0134e-01, 3.3289e-02, 5.9843e-01],\n",
      "         [4.1524e-01, 7.9655e-01, 3.3821e-01],\n",
      "         [7.6605e-02, 4.2206e-02, 7.0957e-01],\n",
      "         [1.0446e-03, 1.2713e-02, 5.7925e-01],\n",
      "         [7.4910e-02, 5.6080e-01, 1.6807e-01],\n",
      "         [9.6134e-01, 8.0798e-02, 3.5981e-01],\n",
      "         [9.7314e-01, 2.0138e-01, 1.4773e-01],\n",
      "         [5.5868e-01, 8.0468e-01, 3.2804e-01],\n",
      "         [5.5868e-01, 8.0468e-01, 3.2804e-01]],\n",
      "\n",
      "        [[4.7824e-01, 9.5985e-01, 6.4402e-01],\n",
      "         [9.6139e-01, 2.8300e-01, 4.8533e-01],\n",
      "         [7.4838e-01, 8.1009e-01, 6.8848e-01],\n",
      "         [6.9693e-01, 5.5532e-01, 4.1061e-01],\n",
      "         [5.9921e-01, 9.6230e-02, 6.4083e-01],\n",
      "         [3.5313e-01, 2.9148e-01, 5.2816e-02],\n",
      "         [4.5691e-01, 1.4899e-01, 8.5660e-01],\n",
      "         [7.7728e-01, 1.6198e-01, 2.7771e-01],\n",
      "         [4.5144e-01, 5.3544e-01, 9.0952e-02],\n",
      "         [8.5440e-01, 3.9522e-01, 1.0814e-01],\n",
      "         [8.5440e-01, 3.9522e-01, 1.0814e-01]],\n",
      "\n",
      "        [[6.0767e-02, 9.6551e-01, 5.3546e-01],\n",
      "         [4.8872e-01, 5.6301e-01, 1.3230e-01],\n",
      "         [1.0029e-01, 2.8204e-01, 4.2783e-01],\n",
      "         [7.7826e-01, 9.8692e-02, 7.2770e-01],\n",
      "         [5.5774e-01, 2.4134e-01, 1.1196e-01],\n",
      "         [8.7913e-02, 7.4240e-01, 5.6761e-01],\n",
      "         [3.7998e-01, 3.7113e-01, 3.8801e-01],\n",
      "         [7.6488e-01, 3.3164e-01, 7.9974e-01],\n",
      "         [3.9285e-01, 1.6142e-01, 8.1081e-01],\n",
      "         [5.2206e-01, 9.3941e-02, 8.8465e-01],\n",
      "         [5.2206e-01, 9.3941e-02, 8.8465e-01]],\n",
      "\n",
      "        [[6.8532e-01, 8.1981e-01, 6.7140e-01],\n",
      "         [4.5248e-01, 3.5534e-02, 3.5688e-01],\n",
      "         [9.1627e-02, 6.7547e-01, 3.6836e-01],\n",
      "         [9.9682e-01, 2.6243e-01, 9.7876e-01],\n",
      "         [1.0590e-01, 1.4437e-01, 7.1354e-01],\n",
      "         [4.1299e-01, 6.6617e-01, 2.8163e-01],\n",
      "         [8.6777e-01, 8.2841e-01, 8.8111e-01],\n",
      "         [3.8900e-01, 7.0850e-01, 1.6905e-01],\n",
      "         [9.7224e-01, 8.1931e-01, 5.5655e-01],\n",
      "         [4.8305e-01, 7.1124e-01, 5.5964e-01],\n",
      "         [4.8305e-01, 7.1124e-01, 5.5964e-01]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.rand(12,10,3)\n",
    "# print(data)\n",
    "print(data[:,-1,:].unsqueeze(1).shape)\n",
    "data = torch.cat((data, data[:,-1,:].unsqueeze(1)),dim=1)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.autonotebook import tqdm\n",
    "import sys\n",
    "# sys.path.append('/scratch/xc1490/projects/tmp/python_packages')\n",
    "import wandb\n",
    "wandb.__version__\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "PROJECT_PATH = os.path.dirname(os.getcwd())\n",
    "FRAME_RATE = 60 # 60 frames/sec\n",
    "MAX_HISTORY_TIME = 10\n",
    "MAX_PREDICTION_TIME = 10\n",
    "HISTORY_TIME = 1.0\n",
    "PREDICTION_TIME = 0.1\n",
    "HISTORY_LENGTH = int(HISTORY_TIME*FRAME_RATE)\n",
    "PREDICTION_LENGTH = int(PREDICTION_TIME*FRAME_RATE)\n",
    "MAX_HISTORY_LENGTH = MAX_HISTORY_TIME*FRAME_RATE\n",
    "MAX_PREDICTION_LENGTH = MAX_PREDICTION_TIME*FRAME_RATE\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: \", DEVICE)\n",
    "TOTAL_FEATURE_NAMES = ['head_x','head_y','head_z','head_r_sin','head_r_cos','head_p_sin','head_p_cos','head_y_sin',\\\n",
    "'head_y_cos','head_rx','head_ry','head_rz']\n",
    "DEFAULT_FEATURE_NAMES = ['head_x','head_y','head_z','head_r_sin','head_r_cos','head_p_sin','head_p_cos','head_y_sin',\\\n",
    "'head_y_cos']\n",
    "XYZ_FEATURE_NAMES = ['head_x', 'head_y', 'head_z']\n",
    "CUSTOM_FEATURES = ['head_x', 'head_y','head_z']\n",
    "ONE_FEATURE = ['head_x']\n",
    "SC_FEATURE_NAMES = ['head_r_sin','head_r_cos','head_p_sin','head_p_cos','head_y_sin','head_y_cos']\n",
    "RPY_FEATURE_NAMES = ['head_rx','head_ry','head_rz']\n",
    "ANGLE_FEATURE_NAMES = ['head_r_cos','head_p_sin','head_p_cos','head_y_sin','head_y_cos','head_rx','head_ry','head_rz']\n",
    "FEATURE_NAMES = XYZ_FEATURE_NAMES\n",
    "FEATURE_INDEX = [TOTAL_FEATURE_NAMES.index(x) for x in FEATURE_NAMES]\n",
    "DEFAULT_FEATURE_SIZE = len(DEFAULT_FEATURE_NAMES)\n",
    "FEATURE_SIZE = len(FEATURE_NAMES)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoVDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, feature_idx):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.x_data = x_data[:,:,feature_idx]\n",
    "        self.y_data = y_data[:,:,feature_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.x_data[idx])\n",
    "        y = torch.tensor(self.y_data[idx])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "processed_data_path = f'{PROJECT_PATH}/processed_data'\n",
    "x_train = np.loadtxt(f'{processed_data_path}/x_train_{HISTORY_TIME}_{PREDICTION_TIME}.csv', dtype='float32', delimiter=',').reshape((-1,HISTORY_LENGTH,DEFAULT_FEATURE_SIZE))\n",
    "y_train = np.loadtxt(f'{processed_data_path}/y_train_{HISTORY_TIME}_{PREDICTION_TIME}.csv', dtype='float32', delimiter=',').reshape((-1,PREDICTION_LENGTH,DEFAULT_FEATURE_SIZE))\n",
    "x_val = np.loadtxt(f'{processed_data_path}/x_val_{HISTORY_TIME}_{PREDICTION_TIME}.csv', dtype='float32', delimiter=',').reshape((-1,HISTORY_LENGTH,DEFAULT_FEATURE_SIZE))\n",
    "y_val = np.loadtxt(f'{processed_data_path}/y_val_{HISTORY_TIME}_{PREDICTION_TIME}.csv', dtype='float32', delimiter=',').reshape((-1,PREDICTION_LENGTH,DEFAULT_FEATURE_SIZE))\n",
    "x_test = np.loadtxt(f'{processed_data_path}/x_test_{HISTORY_TIME}_{PREDICTION_TIME}.csv', dtype='float32', delimiter=',').reshape((-1,HISTORY_LENGTH,DEFAULT_FEATURE_SIZE))\n",
    "y_test = np.loadtxt(f'{processed_data_path}/y_test_{HISTORY_TIME}_{PREDICTION_TIME}.csv', dtype='float32', delimiter=',').reshape((-1,PREDICTION_LENGTH,DEFAULT_FEATURE_SIZE))\n",
    "mean_std = np.loadtxt(f'{processed_data_path}/xyz_mean_std_{HISTORY_TIME}_{PREDICTION_TIME}.csv', dtype='float32', delimiter=',').reshape((3, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "feature_names = FEATURE_NAMES\n",
    "feature_idx = FEATURE_INDEX\n",
    "x_train = x_train[:,:,feature_idx]\n",
    "y_train = y_train[:,:,feature_idx]\n",
    "x_val = x_val[:,:,feature_idx]\n",
    "y_val = y_val[:,:,feature_idx]\n",
    "x_test = x_test[:,:,feature_idx]\n",
    "y_test = y_test[:,:,feature_idx]\n",
    "train_data = FoVDataset(x_train, y_train, feature_idx)\n",
    "val_data = FoVDataset(x_val, y_val, feature_idx)\n",
    "test_data = FoVDataset(x_test, y_test, feature_idx)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = TimeSeriesTransformerConfig(\n",
    "    prediction_length = PREDICTION_LENGTH,\n",
    "    context_length = HISTORY_LENGTH,\n",
    "    input_size = FEATURE_SIZE,\n",
    "    lags_sequence=[0],\n",
    "    num_dynamic_real_features=FEATURE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "file = hf_hub_download(\n",
    "    repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    ")\n",
    "batch = torch.load(file)\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction.from_pretrained(\n",
    "    \"huggingface/time-series-transformer-tourism-monthly\"\n",
    ")\n",
    "\n",
    "mymodel = TimeSeriesTransformerForPrediction(configuration)\n",
    "# mymodel = TimeSeriesTransformerModel(configuration)\n",
    "# during training, one provides both past and future values\n",
    "# as well as possible additional features\n",
    "# outputs = model(\n",
    "#     past_values=batch[\"past_values\"],\n",
    "#     past_time_features=batch[\"past_time_features\"],\n",
    "#     past_observed_mask=batch[\"past_observed_mask\"],\n",
    "#     static_categorical_features=batch[\"static_categorical_features\"],\n",
    "#     static_real_features=batch[\"static_real_features\"],\n",
    "#     future_values=batch[\"future_values\"],\n",
    "#     future_time_features=batch[\"future_time_features\"],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, target, feature_names):\n",
    "    # compute loss for each feature\n",
    "    # output/target: [N, seq_len, 9]\n",
    "    # change sin cos back to angle ?\n",
    "    if 'head_r_sin' in feature_names:\n",
    "        feature_idx = feature_names.index('head_r_sin')\n",
    "        output_rx = torch.atan2(output[:,:,feature_idx], output[:,:,feature_idx+1]).unsqueeze(2)\n",
    "        target_rx = torch.atan2(target[:,:,feature_idx], target[:,:,feature_idx+1]).unsqueeze(2)\n",
    "        output = torch.cat((output,output_rx),-1)\n",
    "    if 'head_p_sin' in feature_names:\n",
    "        feature_idx = feature_names.index('head_p_sin')\n",
    "        output_ry = torch.atan2(output[:,:,feature_idx], output[:,:,feature_idx+1]).unsqueeze(2)\n",
    "        target_ry = torch.atan2(target[:,:,feature_idx], target[:,:,feature_idx+1]).unsqueeze(2)\n",
    "        output = torch.cat((output,output_ry),-1)\n",
    "    if 'head_y_sin' in feature_names:\n",
    "        feature_idx = feature_names.index('head_y_sin')\n",
    "        output_rz = torch.atan2(output[:,:,feature_idx], output[:,:,feature_idx+1]).unsqueeze(2)\n",
    "        target_rz = torch.atan2(target[:,:,feature_idx], target[:,:,feature_idx+1]).unsqueeze(2)\n",
    "        output = torch.cat((output,output_rz),-1)\n",
    "    mse_loss = torch.mean((output - target) ** 2, [0,1])\n",
    "    pearsonr = stats.pearsonr(output.detach().cpu().flatten(), target.detach().cpu().flatten())\n",
    "    return mse_loss, pearsonr\n",
    "\n",
    "def train(device, model: nn.Module, data_loader, optimizer, step, feature_names, plot_flag = False):\n",
    "    progress_bar = tqdm(data_loader)\n",
    "    model.train() # turn on train mode\n",
    "    feature_size = len(feature_names)\n",
    "    num_batches = len(data_loader)\n",
    "    log_interval = num_batches // 5\n",
    "    loss_names = list(map(lambda x: x+'_loss', feature_names))\n",
    "    start_time = time.time()\n",
    "    total_loss = 0.\n",
    "    return_loss = 0.\n",
    "    sep_return_loss = np.zeros((1,feature_size))\n",
    "    loss_dict_train = {}\n",
    "    for batch_idx, (data, targets) in enumerate(progress_bar):\n",
    "#         print(\"batch index: \", batch_idx)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        hist_pe = torch.arange(0,data.shape[1]).repeat(FEATURE_SIZE,1).T.repeat(data.shape[0],1).reshape(data.shape)\n",
    "#         hist_pe = torch.arange(0,data.shape[1]).repeat(data.shape[0],1)\n",
    "        pred_pe = torch.arange(0,targets.shape[1]).repeat(FEATURE_SIZE,1).T.repeat(targets.shape[0],1).reshape(targets.shape)\n",
    "#         pred_pe = torch.arange(0,targets.shape[1]).repeat(targets.shape[0],1)\n",
    "        \n",
    "        output = model(past_values=data,\n",
    "                         past_time_features=hist_pe,\n",
    "                         past_observed_mask=torch.ones(data.shape),\n",
    "                         future_values=targets,\n",
    "                         future_time_features=pred_pe\n",
    "                        )\n",
    "        optimizer.zero_grad()\n",
    "#         sep_mse_loss, train_pearsonr = my_loss(output, targets, feature_names)\n",
    "#         loss = torch.sum(sep_mse_loss[:feature_size])\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        total_loss += loss.item()\n",
    "        return_loss += loss.item()\n",
    "        sep_return_loss += loss.item()\n",
    "        progress_bar.set_postfix_str(f\"training loss={loss.item():.4e}|avg training loss={total_loss/(batch_idx+1):.4e}\")\n",
    "        loss_dict_train['training loss'] = loss.item()\n",
    "#         for name_count, loss_name in enumerate(loss_names):\n",
    "#             loss_dict_train[loss_name] = sep_return_loss[name_count]\n",
    "#         loss_dict_train['avg training loss'] = total_loss/(batch_idx+1)\n",
    "#         loss_dict_train['train pearsonr'] = train_pearsonr\n",
    "        \n",
    "        if use_wandb:\n",
    "            wandb.log(loss_dict_train) \n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print(f'| epoch {epoch:3d} | {batch_idx:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.5f} | ms/batch {ms_per_batch:5.5f} | '\n",
    "                  f'loss {cur_loss:5.5f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            if plot_flag == True:\n",
    "                # training prediction\n",
    "                print ('training prediction')\n",
    "                visualize_data(data.detach().cpu().numpy(), targets.detach().cpu().numpy(), output.detach().cpu().numpy())\n",
    "    return return_loss/(batch_idx+1), sep_return_loss/(batch_idx+1), train_pearsonr\n",
    "\n",
    "\n",
    "def validate(device, model: nn.Module, dataloader: DataLoader, feature_names, plot_flag = False):\n",
    "    feature_size = len(feature_names)\n",
    "    loss_names = list(map(lambda x: x+'_loss', feature_names))\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    sep_total_loss = np.zeros((1,feature_size))\n",
    "    loss_dict_valid = {}\n",
    "    iter_count = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, targets) in dataloader:\n",
    "            data = data.to(device) # [N, seq_len, feature_size]\n",
    "            targets = targets.to(device) # [N, seq_len, feature_size]\n",
    "#             output = model(data)\n",
    "            hist_pe = torch.arange(0,data.shape[1]).repeat(FEATURE_SIZE,1).T.repeat(data.shape[0],1).reshape(data.shape)\n",
    "            pred_pe = torch.arange(0,targets.shape[1]).repeat(FEATURE_SIZE,1).T.repeat(targets.shape[0],1).reshape(targets.shape)\n",
    "            outputs = model.generate(\n",
    "                past_values=data,\n",
    "                past_time_features=hist_pe,\n",
    "                past_observed_mask=torch.ones(data.shape),\n",
    "                future_values=targets,\n",
    "                future_time_features=pred_pe\n",
    "            )\n",
    "            print(outputs)\n",
    "#             if plot_flag and iter_count == 0:\n",
    "#                 print ('validation prediction')\n",
    "#                 visualize_data(data.detach().cpu().numpy(), targets.detach().cpu().numpy(), output.detach().cpu().numpy())\n",
    "#                 if use_wandb:\n",
    "#                     wandb.log({\"validation plot\": fig})\n",
    "#             sep_valid_loss, valid_pearsonr = my_loss(output, targets, feature_names)\n",
    "#             total_loss += torch.sum(sep_valid_loss)\n",
    "#             sep_total_loss += sep_valid_loss.detach().cpu().numpy()\n",
    "#             #import pdb;pdb.set_trace()\n",
    "#             for name_count, loss_name in enumerate(loss_names):\n",
    "#                 loss_dict_valid[loss_name] = sep_valid_loss[name_count]\n",
    "#             #loss_dict_valid['valid loss'] = sep_valid_loss\n",
    "#             loss_dict_valid['valid pearsonr'] = valid_pearsonr[0]\n",
    "#             iter_count += 1\n",
    "#             if use_wandb:\n",
    "#                 wandb.log(loss_dict_valid) \n",
    "    return total_loss/(len(dataloader) - 1), sep_total_loss/(len(dataloader) - 1), valid_pearsonr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training hyperparams\n",
    "in_seq_len = HISTORY_LENGTH\n",
    "out_seq_len = PREDICTION_LENGTH\n",
    "\n",
    "n_heads = 3 ##4\n",
    "head_dim = 3 #32 # dimension of each head, not total\n",
    "dim_val = 9 #16#n_heads*head_dim # embedding dimension, all heads together\n",
    "feature_size = FEATURE_SIZE\n",
    "lr = 0.005\n",
    "tf_rate = 0.5\n",
    "epochs = 200\n",
    "n_decoder_layers = 2\n",
    "n_encoder_layers = 2\n",
    "batch_size = BATCH_SIZE\n",
    "n_batches = len(train_dataloader)\n",
    "model = mymodel\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "#keep track of loss for graph\n",
    "train_mse_losses = []\n",
    "val_mse_losses = []\n",
    "test_mse_losses = []\n",
    "sep_train_mse_losses = []\n",
    "sep_val_mse_losses = []\n",
    "sep_test_mse_losses = []\n",
    "train_pearsonr_arr = []\n",
    "val_pearsonr_arr = []\n",
    "test_pearsonr_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e8ea6cd3164c829651c2b6a42a8fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c46418830e4e619911311fb4543fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    84/  423 batches | lr 0.00500 | ms/batch 2363.54075 | loss -0.33710\n",
      "training prediction\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'visualize_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 22\u001b[0m     train_loss, sep_train_loss, train_pearsonr \u001b[38;5;241m=\u001b[39m train(DEVICE, model, train_dataloader, optimizer, step, feature_names, plot_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     print(train_loss)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     train_mse_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[9], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(device, model, data_loader, optimizer, step, feature_names, plot_flag)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m plot_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;66;03m# training prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining prediction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m             visualize_data(data\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), targets\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_loss\u001b[38;5;241m/\u001b[39m(batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), sep_return_loss\u001b[38;5;241m/\u001b[39m(batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), train_pearsonr\n",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Trainig\n",
    "#build live matplotlib fig\n",
    "\n",
    "\n",
    "#plt.ion()\n",
    "#fig.show()\n",
    "#fig.canvas.draw()\n",
    "epochs = 20\n",
    "LOAD_MODEL = False\n",
    "use_wandb = False\n",
    "if LOAD_MODEL:\n",
    "    load_ckpt(f\"{PROJECT_PATH}/checkpoints/batch_{BATCH_SIZE}_{HISTORY_TIME}_{PREDICTION_TIME}_ckpts.pt\".format(os.getcwd(), epochs, batch_size), model, optimizer)\n",
    "lr = 0.1\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "# print(scaler)\n",
    "best_val_loss = float('inf')\n",
    "#writer = SummaryWriter(\"run/loss_plot\")\n",
    "step = 0\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    print(f'Epoch #{epoch}')\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss, sep_train_loss, train_pearsonr = train(DEVICE, model, train_dataloader, optimizer, step, feature_names, plot_flag=True if epoch % 2 == 1 else False)\n",
    "#     print(train_loss)\n",
    "    train_mse_losses.append(train_loss)\n",
    "    sep_train_mse_losses.append(sep_train_loss)\n",
    "    mean_loss = sum(train_mse_losses)/len(train_mse_losses)\n",
    "    print(\"validation\")\n",
    "    val_loss, sep_val_loss, val_pearsonr = validate(DEVICE, model, val_dataloader, feature_names, plot_flag=True if epoch % 2 == 1 else False)\n",
    "    val_mse_losses.append(val_loss)\n",
    "    sep_val_mse_losses.append(sep_val_loss)\n",
    "    test_loss, sep_test_loss, test_pearsonr = validate(DEVICE, model, test_dataloader, feature_names, plot_flag=True if epoch % 2 == 1 else False)\n",
    "    test_mse_losses.append(test_loss)\n",
    "    sep_test_mse_losses.append(sep_test_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:5.4f} | mean loss {mean_loss:8.4f}')\n",
    "    print('-' * 89)\n",
    "    if epoch % 2 == 1:\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "        #ax.plot([i.detach().cpu().numpy() for i in test_losses], label='train loss')\n",
    "        ax[0].plot([i.detach().cpu().numpy() for i in val_mse_losses[5:]] , label='validation loss')\n",
    "        ax[0].plot([i  for i in train_mse_losses[5:]] , label='train loss')\n",
    "        ax[0].set_title(\"Losses\")\n",
    "\n",
    "        ax[0].legend()\n",
    "        ax[1].plot([i.detach().cpu().numpy() for i in test_mse_losses], label='test loss')\n",
    "        ax[1].set_title(\"Losses\")\n",
    "        ax[1].legend()\n",
    "        #fig.canvas.draw()\n",
    "        plt.show()\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # save model\n",
    "#         save_ckpt(\"{}/checkpoints/epoch_{}_ckpts.pt\".format(os.getcwd(), epoch), model, optimizer,epochs, val_loss)\n",
    "        save_dict = {\n",
    "            'epoch': epoch,\n",
    "            'train_mse_losses': train_mse_losses,\n",
    "            'val_mse_losses': val_mse_losses\n",
    "        }\n",
    "        save_ckpt(f\"{PROJECT_PATH}/checkpoints\", model, optimizer, save_dict)\n",
    "\n",
    "    # scheduler.step(mean_loss)\n",
    "    scheduler.step()\n",
    "# load_ckpt(\"/content/drive/MyDrive/checkpoints.pt\", model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformerConfig {\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"cardinality\": [\n",
       "    0\n",
       "  ],\n",
       "  \"context_length\": 60,\n",
       "  \"d_model\": 64,\n",
       "  \"decoder_attention_heads\": 2,\n",
       "  \"decoder_ffn_dim\": 32,\n",
       "  \"decoder_layerdrop\": 0.1,\n",
       "  \"decoder_layers\": 2,\n",
       "  \"distribution_output\": \"student_t\",\n",
       "  \"dropout\": 0.1,\n",
       "  \"embedding_dimension\": [\n",
       "    0\n",
       "  ],\n",
       "  \"encoder_attention_heads\": 2,\n",
       "  \"encoder_ffn_dim\": 32,\n",
       "  \"encoder_layerdrop\": 0.1,\n",
       "  \"encoder_layers\": 2,\n",
       "  \"feature_size\": 24,\n",
       "  \"init_std\": 0.02,\n",
       "  \"input_size\": 3,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"lags_sequence\": [\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0\n",
       "  ],\n",
       "  \"loss\": \"nll\",\n",
       "  \"model_type\": \"time_series_transformer\",\n",
       "  \"num_dynamic_real_features\": 0,\n",
       "  \"num_parallel_samples\": 100,\n",
       "  \"num_static_categorical_features\": 0,\n",
       "  \"num_static_real_features\": 0,\n",
       "  \"num_time_features\": 0,\n",
       "  \"prediction_length\": 6,\n",
       "  \"scaling\": \"mean\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true\n",
       "}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.eval()\n",
    "total_loss, sep_loss, pearsonr = validate(DEVICE, model, val_dataloader,XYZ_FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 61])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"past_values\"].shape #[batch_size, num_batch, feature_size]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1364,  2.2175],\n",
       "         [ 0.2273,  2.2201],\n",
       "         [ 0.3182,  2.2227],\n",
       "         ...,\n",
       "         [-0.0455,  2.3483],\n",
       "         [ 0.0455,  2.3502],\n",
       "         [ 0.1364,  2.3522]],\n",
       "\n",
       "        [[ 0.1364,  2.1847],\n",
       "         [ 0.2273,  2.1875],\n",
       "         [ 0.3182,  2.1903],\n",
       "         ...,\n",
       "         [-0.0455,  2.3243],\n",
       "         [ 0.0455,  2.3263],\n",
       "         [ 0.1364,  2.3284]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.2273,  0.6990],\n",
       "         [-0.1364,  0.7782],\n",
       "         [-0.0455,  0.8451]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.4091,  1.5563],\n",
       "         [ 0.5000,  1.5682],\n",
       "         [-0.5000,  1.5798]],\n",
       "\n",
       "        [[-0.4091,  1.7993],\n",
       "         [-0.3182,  1.8062],\n",
       "         [-0.2273,  1.8129],\n",
       "         ...,\n",
       "         [ 0.5000,  2.0828],\n",
       "         [-0.5000,  2.0864],\n",
       "         [-0.4091,  2.0899]],\n",
       "\n",
       "        [[ 0.5000,  1.1139],\n",
       "         [-0.5000,  1.1461],\n",
       "         [-0.4091,  1.1761],\n",
       "         ...,\n",
       "         [ 0.3182,  1.8513],\n",
       "         [ 0.4091,  1.8573],\n",
       "         [ 0.5000,  1.8633]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"past_time_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"past_observed_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"static_categorical_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"static_real_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"future_time_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"future_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformerConfig {\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"cardinality\": [\n",
       "    0\n",
       "  ],\n",
       "  \"context_length\": 60,\n",
       "  \"d_model\": 64,\n",
       "  \"decoder_attention_heads\": 2,\n",
       "  \"decoder_ffn_dim\": 32,\n",
       "  \"decoder_layerdrop\": 0.1,\n",
       "  \"decoder_layers\": 2,\n",
       "  \"distribution_output\": \"student_t\",\n",
       "  \"dropout\": 0.1,\n",
       "  \"embedding_dimension\": [\n",
       "    0\n",
       "  ],\n",
       "  \"encoder_attention_heads\": 2,\n",
       "  \"encoder_ffn_dim\": 108,\n",
       "  \"encoder_layerdrop\": 0.1,\n",
       "  \"encoder_layers\": 2,\n",
       "  \"feature_size\": 9,\n",
       "  \"init_std\": 0.02,\n",
       "  \"input_size\": 3,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"lags_sequence\": [\n",
       "    0\n",
       "  ],\n",
       "  \"loss\": \"nll\",\n",
       "  \"model_type\": \"time_series_transformer\",\n",
       "  \"num_dynamic_real_features\": 0,\n",
       "  \"num_parallel_samples\": 100,\n",
       "  \"num_static_categorical_features\": 0,\n",
       "  \"num_static_real_features\": 0,\n",
       "  \"num_time_features\": 0,\n",
       "  \"prediction_length\": 6,\n",
       "  \"scaling\": \"mean\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true\n",
       "}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fov_env",
   "language": "python",
   "name": "fov_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
